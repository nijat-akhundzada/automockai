# AutoMockAI üöÄ

**A schema-aware, AI-powered mock data generator for relational databases.**

AutoMockAI connects to your database, analyzes its schema, and generates realistic, context-aware mock data. It features a full AI pipeline, allowing you to train a custom model on sample datasets for high-quality data generation.

---

## ‚ú® Features

-   **Custom AI Model Training**: Includes scripts to preprocess datasets and train a lightweight transformer model (`distilgpt2`) to generate context-aware data.
-   **Advanced Schema Analysis**: Automatically detects tables, columns, types, primary keys, foreign keys, and constraints for PostgreSQL, MySQL, and SQLite.
-   **Tiered Generation Strategy**:
    1.  **Custom AI Model**: Prioritizes the fine-tuned local model for the most accurate and context-aware data.
    2.  **Ollama Fallback**: Can use a local LLM (via Ollama) if the custom model is unavailable.
    3.  **Faker Fallback**: Guarantees type-correct data generation using [Faker](https://faker.readthedocs.io/) as a final fallback.
-   **Relational Integrity**:
    -   **Dependency-Aware Insertion**: Inserts data in a topologically sorted order to respect foreign key constraints.
    -   **Valid Foreign Key References**: Populates foreign key columns with values that exist in the referenced parent table.
-   **Constraint Enforcement**: Honors `NOT NULL`, `UNIQUE`, and `CHECK` constraints during data generation.
-   **Transactional & Batched Inserts**: Inserts data in batches within a single transaction for performance and safety.
-   **Data Validation**: Includes a validation agent to assess the quality and integrity of the inserted data.
-   **Flexible CLI**: A powerful Typer-based CLI for fine-grained control over the entire workflow.

---

## üèóÔ∏è Architecture

AutoMockAI operates through a sequence of specialized agents:

1.  **CLI Orchestrator**: The user entrypoint that manages the `preprocess`, `train`, and `generate` commands.
2.  **Preprocessing Agent**: Downloads and transforms sample datasets into a unified format for training.
3.  **Training Agent**: Fine-tunes the transformer model on the preprocessed data.
4.  **Schema Analyzer**: Connects to the database and builds a detailed, normalized schema map.
5.  **Data Generator**: Generates mock data using the tiered AI strategy (Custom Model ‚Üí Ollama ‚Üí Faker).
6.  **Data Inserter**: Inserts the generated data into the database transactionally.
7.  **Data Validator**: (Optional) Evaluates the quality of the inserted data.

---

## üìÇ Project Structure

This project is organized into the following directories and key files:

-   `.git/`: Git version control system files.
-   `.pytest_cache/`: Cache directory used by `pytest` for faster test execution.
-   `.venv/`: Python virtual environment, containing installed dependencies and project executables.
-   `.windsurf/`: (Purpose unknown without further context. Potentially related to a specific development environment or tool.)
-   `commands.txt`: (Purpose unknown without further context. Potentially a script or list of commands.)
-   `docker-compose.ollama.yml`: Docker Compose configuration for running Ollama, likely for the LLM fallback.
-   `LICENSE`: The project's license (MIT License).
-   `Project description AutoMockAI.pdf`: The detailed project description document.
-   `pyproject.toml`: Project configuration file, including dependencies and build system settings.
-   `README.md`: This overview document.
-   `uv.lock`: Dependency lock file generated by `uv` to ensure reproducible installations.
-   `src/`: Contains the main source code for the `automockai` package.
    -   `automockai/`: The core Python package.
        -   `__init__.py`: Initializes the Python package.
        -   `cli.py`: Defines the command-line interface (CLI) using `typer`, orchestrating various project commands (`preprocess`, `train`, `generate`).
        -   `fallback.py`: Implements fallback data generation strategies, such as using the `Faker` library when AI models are unavailable or unsuitable.
        -   `generator.py`: Contains the logic for generating mock data based on schema analysis and AI models.
        -   `inserter.py`: Handles the insertion of generated mock data into the target database, ensuring relational integrity.
        -   `py.typed`: A marker file indicating that the package supports type hints.
        -   `schema.py`: Responsible for connecting to the database, analyzing its schema (tables, columns, types, constraints, foreign keys), and building a normalized schema map.
        -   `model/`: Directory containing components related to the AI model.
            -   `preprocessing.py`: Scripts for downloading, cleaning, and transforming raw datasets into a format suitable for model training.
            -   `train.py`: Contains the logic for fine-tuning the `distilgpt2` transformer model on the preprocessed data.
            -   `evaluate.py`: Implements the `DataValidator` class, which assesses the quality and integrity of inserted data against database constraints.
-   `tests/`: Contains unit and integration tests for the project's components.

---

## üì¶ Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/automockai.git
cd automockai

# Create a virtual environment and install dependencies
uv venv
source .venv/bin/activate
uv pip install -e '.'
```

*Note: If the `automockai` command is not directly available after activation, you might need to use `.venv/bin/automockai` instead (e.g., `.venv/bin/automockai preprocess`).*

---

## üîß Demonstration Workflow

Follow these steps to preprocess data, train the model, and generate mock data.

### 1. Preprocess the Data

This command downloads the required datasets and creates a `training_data.csv` file.

```bash
automockai preprocess
```

*Note: Automatic download of the Northwind dataset may fail. If so, please manually download a Northwind CSV dataset (e.g., from GitHub) and extract its contents into the `data/northwind` directory. The Kaggle datasets also require authentication. Please ensure your `kaggle.json` is placed in `~/.kaggle/` or follow the CLI prompts to download the files manually. If no datasets are successfully processed, `training_data.csv` will be empty, but the `train` command can still be run for demonstration purposes.*

### 2. Train the AI Model

This command fine-tunes the `distilgpt2` model on the data from the previous step. The trained model will be saved to `model/trained_model/`.

```bash
automockai train
```

### 3. Generate Mock Data

Before generating data, you need a target database. For a quick demonstration, you can use SQLite. Create a `test.db` file and define your schema (e.g., using SQL DDL statements).

**Example SQLite DSN:** `sqlite:///./test.db`

For PostgreSQL, ensure your database is running and you have a user with appropriate permissions.

**Example PostgreSQL DSN:** `postgresql+psycopg://user:pass@localhost:5432/mydb`

Now you can use the `generate` command. It will automatically load and use your custom-trained model.

```bash
automockai generate \
  --dsn "sqlite:///./test.db" \
  --count 10 \
  --include "users,products" \
  --validate
```

This command will connect to your SQLite database, analyze its schema, generate 10 rows for 'users' and 'products' tables using the trained AI model, insert them, and then run validation checks.


---

## ‚öôÔ∏è CLI Reference

### `generate` Command Options

| Option                | Short | Description                                                              |
| --------------------- | ----- | ------------------------------------------------------------------------ |
| `--dsn`               |       | **(Required)** Database connection string (DSN).                         |
| `--count`             | `-c`  | Number of rows to generate per table (default: 10).                      |
| `--include`           | `-i`  | Comma-separated glob patterns of tables to include.                      |
| `--exclude`           | `-e`  | Comma-separated glob patterns of tables to exclude.                      |
| `--use-local-model`   |       | Use the local fine-tuned model for generation (default: True).           |
| `--fallback-only`     |       | Use only the Faker-based fallback, skipping all AI models.               |
| `--dry-run`           |       | Generate data but do not insert it.                                      |
| `--validate`          |       | Run validation checks on the generated data after insertion.             |
| `--output`            | `-o`  | Path to save generated data as JSON (requires `--dry-run`).            |

### Other Commands

-   `automockai preprocess`: See step 1.
-   `automockai train`: See step 2.

---

## üìú License

MIT License ‚Äì see [LICENSE](./LICENSE).