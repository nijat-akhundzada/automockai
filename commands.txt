docker exec -it ollama ollama pull mistral

# Using Ollama (Docker on localhost):

automockai \
 --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
 --provider ollama \
 --model mistral \
 --rows 5 \
 --dry-run \
 --only-prefix injuria_

 automockai \
  --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
  --provider ollama --model mistral --rows 20 --dry-run \
  --only-prefix injuria_ \
  --exclude "injuria_post,injuria_event"

# Using Faker (no LLM needed):

automockai \
 --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
 --provider faker \
 --rows 20

automockai \
 --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
 --provider faker --rows 10 \
 --include "injuria_*,custom_*"

automockai \
 --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
 --provider faker --rows 10 \
 --include "injuria*\*,custom*\*"

# Using OpenAI-compatible (self-host or cloud):

export OPENAI_API_KEY=sk-...
automockai \
 --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
 --provider openai \
 --model gpt-4o-mini \
 --rows 20 \
 --dry-run

# Environment variables

export OLLAMA_HOST=http://localhost:11434


How to use

Dry-run (preview only), focusing on your app tables:

export OLLAMA_HOST=http://localhost:11434
automockai \
  --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
  --provider ollama --model mistral \
  --rows 5 --dry-run --only-prefix injuria_


Execute and commit per table (no waiting for all):

automockai \
  --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
  --provider ollama --model mistral \
  --rows 2 --execute --only-prefix injuria_


Faker (no LLM) â€” also commits per table with --execute:

automockai \
  --dsn postgresql+psycopg://devuser:changeme@127.0.0.1/devdb \
  --provider faker --rows 50 --execute --only-prefix injuria_